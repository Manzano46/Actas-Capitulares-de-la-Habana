{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline \n",
    "- Insertar Imagen\n",
    "- Procesar Imagen\n",
    "- Separar en líneas\n",
    "- Enviar a los distintos OCRs\n",
    "- Correcciones \n",
    "- Decisión Final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insertar Imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "input_folder = \"data/input\"\n",
    "os.makedirs(input_folder, exist_ok=True)\n",
    "output_folder = \"data/output_preprocessing\"\n",
    "os.makedirs(output_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesar Imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento completo.\n"
     ]
    }
   ],
   "source": [
    "from Preprocessing.pipeline2 import cleanup_image\n",
    "\n",
    "cleanup_image(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separar en líneas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviar a los distintos OCRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\53552\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\53552\\Desktop\\Comp. Science\\4to año\\Machine Learning\\Proyecto Kevin\\Actas-Capitulares-de-la-Habana\\Recognition\\SimpleHTR\\SimpleHTR\\src\\model.py:11: The name tf.disable_eager_execution is deprecated. Please use tf.compat.v1.disable_eager_execution instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\53552\\Desktop\\Comp. Science\\4to año\\Machine Learning\\Proyecto Kevin\\Actas-Capitulares-de-la-Habana\\Recognition\\SimpleHTR\\SimpleHTR\\src\\model.py:148: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\53552\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\version_utils.py:76: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\53552\\Desktop\\Comp. Science\\4to año\\Machine Learning\\Proyecto Kevin\\Actas-Capitulares-de-la-Habana\\Recognition\\SimpleHTR\\SimpleHTR\\src\\model.py:73: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  conv_norm = tf.compat.v1.layers.batch_normalization(conv, training=self.is_train)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\53552\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:964: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\53552\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\normalization\\batch_normalization.py:883: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From c:\\Users\\53552\\Desktop\\Comp. Science\\4to año\\Machine Learning\\Proyecto Kevin\\Actas-Capitulares-de-la-Habana\\Recognition\\SimpleHTR\\SimpleHTR\\src\\model.py:94: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\Users\\53552\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:441: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\Users\\53552\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\legacy_cells.py:1043: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\53552\\Desktop\\Comp. Science\\4to año\\Machine Learning\\Proyecto Kevin\\Actas-Capitulares-de-la-Habana\\Recognition\\SimpleHTR\\SimpleHTR\\src\\model.py:86: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  cells = [tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=num_hidden, state_is_tuple=True) for _ in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.5 (tags/v3.11.5:cce6ba9, Aug 24 2023, 14:38:34) [MSC v.1936 64 bit (AMD64)]\n",
      "Tensorflow: 2.15.1\n",
      "Init with stored values from Recognition/SimpleHTR/SimpleHTR/model/snapshot-13\n",
      "INFO:tensorflow:Restoring parameters from Recognition/SimpleHTR/SimpleHTR/model/snapshot-13\n",
      "Ejecutando OCR con el modelo bdd-wormser-scriptorium-abbreviated-0.2 sobre la imagen data/output_preprocessing/binarized.png\n",
      "Ejecutando OCR con el modelo McCATMuS_nfd_nofix_V1 sobre la imagen data/output_preprocessing/binarized.png\n",
      "OCR completado para todas las imágenes segmentadas.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List\n",
    "from Recognition.Kraken.kraken_model import get_transcription\n",
    "\n",
    "processed_photos = os.listdir(output_folder)\n",
    "\n",
    "transcriptions = []\n",
    "\n",
    "for photo in processed_photos:\n",
    "    transcriptions.append(get_transcription(output_folder + '/' + photo))\n",
    "    \n",
    "# output_ocr = 'data/output_ocr'\n",
    "# transcriptions = os.listdir(output_ocr)\n",
    "# results_from_ocrs:List[List[List[str]]] = []\n",
    "\n",
    "ocr_results = []\n",
    "\n",
    "# Para cada imagen tenemos una transcripcion\n",
    "for image in transcriptions:\n",
    "    lines = []\n",
    "    for model in image:         # La transcripcion viene del modo model: x text: y\n",
    "        text = model['text']\n",
    "        lines = text.split('\\n')        # Separamos el texto por lineas\n",
    "    ocr_results.append(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m frequency_dictionary_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization/spanish_frequency_dictionary.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Cargar modelo SymSpell y Spacy\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m nlp, sym_spell \u001b[38;5;241m=\u001b[39m \u001b[43mload_corrector_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspacy_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspacy_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfrequency_dictionary_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrequency_dictionary_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_edit_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefix_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\n\u001b[0;32m     30\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m gemini_model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgemini-2.0-flash-exp\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Crear el sistema de instrucciones para Gemini\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\53552\\Desktop\\Comp. Science\\4to año\\Machine Learning\\Proyecto Kevin\\Actas-Capitulares-de-la-Habana\\serialization\\serialization.py:81\u001b[0m, in \u001b[0;36mload_corrector_model\u001b[1;34m(spacy_model, frequency_dictionary_path, max_edit_distance, prefix_length)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_corrector_model\u001b[39m(spacy_model: \u001b[38;5;28mstr\u001b[39m, frequency_dictionary_path: \u001b[38;5;28mstr\u001b[39m, max_edit_distance: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, prefix_length: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m):\n\u001b[1;32m---> 81\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspacy_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     sym_spell \u001b[38;5;241m=\u001b[39m SymSpell(max_dictionary_edit_distance\u001b[38;5;241m=\u001b[39mmax_edit_distance, prefix_length\u001b[38;5;241m=\u001b[39mprefix_length)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sym_spell\u001b[38;5;241m.\u001b[39mload_dictionary(frequency_dictionary_path, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, separator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\53552\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvocab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m        \u001b[49m\u001b[43menable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\53552\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\spacy\\util.py:472\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE941\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname, full\u001b[38;5;241m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[38;5;66;03m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 472\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE050\u001b[38;5;241m.\u001b[39mformat(name\u001b[38;5;241m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "from serialization.serialization import correction, combine, load_corrector_model\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# results_from_ocr = [\n",
    "#     [\n",
    "#         \"Essta es una prueva de ectraccion de teexto. Connoscida cosa sea a todos los queesta carta uieren como yo don fffffff por la gracia de dios hey de Castiella.\",\n",
    "#         \"Asta carta fue escrita en el anno del senyor de 1493, y  cosas de gran ymportancia para los reynos.\"\n",
    "#     ],\n",
    "#     [\n",
    "#         \"Essa s una prsueva de etraccion de . Connoda cosa sea a tdos as los qeesta carta ueren como yo don Fferrando por la gracia de d hey de Castiella.\",\n",
    "#         \"Asta s fue escrita en el ano de seyor de 143, y consigna cossdas de gran ymportana para los reynos.\"\n",
    "#     ],\n",
    "# ]\n",
    "\n",
    "# Cargar claves de entorno para Gemini\n",
    "load_dotenv()\n",
    "GENAI_API_KEY = os.getenv(\"GENAI_API_KEY\")\n",
    "genai.configure(api_key=GENAI_API_KEY)\n",
    "\n",
    "spacy_model = \"es_core_news_sm\"\n",
    "frequency_dictionary_path = \"serialization/spanish_frequency_dictionary.txt\"\n",
    "\n",
    "# Cargar modelo SymSpell y Spacy\n",
    "nlp, sym_spell = load_corrector_model(\n",
    "    spacy_model=spacy_model,\n",
    "    frequency_dictionary_path=frequency_dictionary_path,\n",
    "    max_edit_distance=3,\n",
    "    prefix_length=7\n",
    ")\n",
    "\n",
    "gemini_model_name=\"gemini-2.0-flash-exp\"\n",
    "\n",
    "# Crear el sistema de instrucciones para Gemini\n",
    "chat = genai.GenerativeModel(\n",
    "    model_name=gemini_model_name,\n",
    "    system_instruction=[\n",
    "        \"\"\"\n",
    "        Eres un asistente que ayuda a refinar textos. Analiza el siguiente texto original y corregido,\n",
    "        y reescríbelo en un formato limpio, coherente y gramaticalmente correcto. Los textos son documentos históricos\n",
    "        extraídos mediante OCR, por lo que pueden contener errores típicos del procesamiento OCR. Los documentos son\n",
    "        cartas en Cuba de los siglos XV y XVI, manten el español antiguo.\n",
    "        \"\"\"\n",
    "    ]\n",
    ").start_chat(history=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Esta es una prueba de extracción de texto. Conocida cosa sea a todos los que esta carta vieren, como yo, don [nombre ilegible], por la gracia de Dios, [rey/señor] de Castilla.', 'Esta carta fue escrita en el año del Señor de 1493, y trata de cosas de gran importancia para los reinos.'], ['Esta es una prueba de extracción. Conocida cosa sea a todos los que esta carta vieren, como yo, don Ferrando, por la gracia de Dios, [rey/señor] de Castilla.', 'Esta fue escrita en el año del Señor de 1493, y consigna cosas de gran importancia para los reinos.']]\n"
     ]
    }
   ],
   "source": [
    "results_from_corrections: List[List[str]] = []\n",
    "\n",
    "for document in ocr_results:\n",
    "    aux: List[str] = []\n",
    "    for line in document:\n",
    "        aux.append(correction(line, nlp, sym_spell, chat))\n",
    "    results_from_corrections.append(aux)\n",
    "    \n",
    "print(results_from_corrections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decisión Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT FINAL\n",
      "['Conocida cosa sea a todos los que esta carta vieren, como yo, don Ferrando, por la gracia de Dios, [rey/señor] de Castilla.', 'Esta fue escrita en el año del Señor de 1493, y consigna cosas de gran importancia para los reinos.']\n"
     ]
    }
   ],
   "source": [
    "result:List[str] = []\n",
    "\n",
    "# Crear el modelo generativo y configurar el sistema de instrucciones\n",
    "chat2 = genai.GenerativeModel(\n",
    "    model_name=gemini_model_name,\n",
    "    system_instruction=\"\"\"\n",
    "    Eres un asistente que ayuda a refinar textos. Analiza las siguientes representaciones de una línea de texto,\n",
    "    y reescribe solo una en un formato limpio, coherente y gramaticalmente correcto. Los textos son documentos históricos\n",
    "    extraídos mediante OCR, por lo que pueden contener errores típicos del procesamiento OCR. Los documentos son\n",
    "    cartas en Cuba de los siglos XV y XVI, manten el español antiguo en tu respuesta.\n",
    "    \"\"\"\n",
    ").start_chat(history=[])\n",
    "\n",
    "for it in range(len(ocr_results[0])):\n",
    "    aux: List[str] = []\n",
    "    for i in range(len(ocr_results)):\n",
    "        aux.append(results_from_corrections[i][it])\n",
    "    \n",
    "    result.append(combine(aux, chat2))\n",
    "\n",
    "print(\"RESULT FINAL\")\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
